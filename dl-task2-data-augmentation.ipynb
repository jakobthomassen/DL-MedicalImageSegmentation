{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe02da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from model import UNet\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c192014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n\n",
    "# CONFIGURATION (Updated for Task 2)\n",
    "# ============================================================================\\n\n",
    "train_file = 'train.txt'\n",
    "val_file = 'val.txt'\n",
    "\n",
    "class Config:\n",
    "    # Dataset paths\n",
    "    DATASET_PATH = \"data\\\\kvasir-seg\"\n",
    "    IMAGE_DIR = \"images\"\n",
    "    MASK_DIR = \"masks\"\n",
    "    \n",
    "    # Experiment parameters\n",
    "    # RESOLUTIONS = [512, 256, 128, 64] # <-- Removed/Commented out (Task 1 only)\n",
    "    TARGET_SIZE = 256\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Early stopping\n",
    "    EARLY_STOPPING_PATIENCE = 5 \n",
    "    # LR Scheduler\n",
    "    SCHEDULER_PATIENCE = 3 \n",
    "    SCHEDULER_FACTOR = 0.1 \n",
    "    \n",
    "    # Model parameters\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 1\n",
    "    FEATURES = [64, 128, 256, 512]\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfb43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n\n",
    "# DATASET CLASS (Updated for Task 2 with Albumentations)\n",
    "# ============================================================================\\n\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, target_size=256, use_augmentations=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        \n",
    "        # --- Define Augmentation Pipelines ---\n",
    "        \n",
    "        # Validation/Base pipeline (just resize)\n",
    "        val_transform = [\n",
    "            A.Resize(height=target_size, width=target_size, \n",
    "                     interpolation=cv2.INTER_LINEAR),\n",
    "            ToTensorV2() # Converts image to (C,H,W) tensor and scales, \n",
    "                         # converts mask to (H,W) int64 tensor\n",
    "        ]\n",
    "        \n",
    "        if use_augmentations:\n",
    "            # Training pipeline (resize + augs)\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(height=target_size, width=target_size, \n",
    "                         interpolation=cv2.INTER_LINEAR),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Rotate(limit=30, interpolation=cv2.INTER_LINEAR, \n",
    "                         border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "                A.ColorJitter(brightness=0.3, contrast=0.3, \n",
    "                              saturation=0.3, hue=0.1, p=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            # Validation pipeline\n",
    "            self.transform = A.Compose(val_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask as numpy arrays\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert('L'))\n",
    "        \n",
    "        # Binarize mask (0 or 1) *before* transforms\n",
    "        # Albumentations expects uint8 mask with class indices\n",
    "        mask = (mask > 128).astype(np.uint8) # 0 for background, 1 for polyp\n",
    "        \n",
    "        # Apply transforms\n",
    "        # 'image' will be key for image, 'mask' for mask\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        \n",
    "        image_tensor = augmented['image']\n",
    "        mask_tensor = augmented['mask']\n",
    "        \n",
    "        # Add channel dimension to mask (H, W) -> (1, H, W)\n",
    "        # and convert from int64 to float for the loss function\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).float()\n",
    "        \n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d58a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 700 images, Val: 300 images\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATH COLLECTION AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple file existence check\n",
    "if not os.path.exists('train.txt') or not os.path.exists('val.txt'):\n",
    "    raise FileNotFoundError('train.txt or val.txt missing.')\n",
    "\n",
    "base_path = Path(config.DATASET_PATH)\n",
    "image_paths = sorted(list((base_path / config.IMAGE_DIR).glob('*.jpg')))\n",
    "mask_paths = sorted(list((base_path / config.MASK_DIR).glob('*.jpg')))\n",
    "\n",
    "if len(image_paths) != len(mask_paths):\n",
    "    raise ValueError(\"Mismatch between number of images and masks.\")\n",
    "\n",
    "# Read train/val lists\n",
    "with open('train.txt', 'r') as f:\n",
    "    train_stems = {line.strip() for line in f}\n",
    "with open('val.txt', 'r') as f:\n",
    "    val_stems = {line.strip() for line in f}\n",
    "\n",
    "# Split dataset according to txt files\n",
    "train_images = [p for p in image_paths if p.stem in train_stems]\n",
    "train_masks = [p for p in mask_paths if p.stem in train_stems]\n",
    "val_images = [p for p in image_paths if p.stem in val_stems]\n",
    "val_masks = [p for p in mask_paths if p.stem in val_stems]\n",
    "\n",
    "print(f\"Train: {len(train_images)} images, Val: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0814aa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Input shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL CALL\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_call():\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "    print(\"Model created successfully!\")\n",
    "    \n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "test_model_call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979e6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channel=3, out_channel=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for feature in features: \n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        for features in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(features*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "            \n",
    "\n",
    "                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afe02da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from model import UNet\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c192014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n\n",
    "# CONFIGURATION (Updated for Task 2)\n",
    "# ============================================================================\\n\n",
    "train_file = 'train.txt'\n",
    "val_file = 'val.txt'\n",
    "\n",
    "class Config:\n",
    "    # Dataset paths\n",
    "    DATASET_PATH = \"data\\\\kvasir-seg\"\n",
    "    IMAGE_DIR = \"images\"\n",
    "    MASK_DIR = \"masks\"\n",
    "    \n",
    "    # Experiment parameters\n",
    "    # RESOLUTIONS = [512, 256, 128, 64] # <-- Removed/Commented out (Task 1 only)\n",
    "    TARGET_SIZE = 256\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Early stopping\n",
    "    EARLY_STOPPING_PATIENCE = 5 \n",
    "    # LR Scheduler\n",
    "    SCHEDULER_PATIENCE = 3 \n",
    "    SCHEDULER_FACTOR = 0.1 \n",
    "    \n",
    "    # Model parameters\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 1\n",
    "    FEATURES = [64, 128, 256, 512]\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcfb43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\\n\n",
    "# DATASET CLASS (Updated for Task 2 with Albumentations)\n",
    "# ============================================================================\\n\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, target_size=256, use_augmentations=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        \n",
    "        # --- Define Augmentation Pipelines ---\n",
    "        \n",
    "        # Validation/Base pipeline (just resize)\n",
    "        val_transform = [\n",
    "            A.Resize(height=target_size, width=target_size, \n",
    "                     interpolation=cv2.INTER_LINEAR),\n",
    "            ToTensorV2() # Converts image to (C,H,W) tensor and scales, \n",
    "                         # converts mask to (H,W) int64 tensor\n",
    "        ]\n",
    "        \n",
    "        if use_augmentations:\n",
    "            # Training pipeline (resize + augs)\n",
    "            self.transform = A.Compose([\n",
    "                A.Resize(height=target_size, width=target_size, \n",
    "                         interpolation=cv2.INTER_LINEAR),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.Rotate(limit=30, interpolation=cv2.INTER_LINEAR, \n",
    "                         border_mode=cv2.BORDER_CONSTANT, p=0.5),\n",
    "                A.ColorJitter(brightness=0.3, contrast=0.3, \n",
    "                              saturation=0.3, hue=0.1, p=0.5),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        else:\n",
    "            # Validation pipeline\n",
    "            self.transform = A.Compose(val_transform)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask as numpy arrays\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert('L'))\n",
    "        \n",
    "        # Binarize mask (0 or 1) *before* transforms\n",
    "        # Albumentations expects uint8 mask with class indices\n",
    "        mask = (mask > 128).astype(np.uint8) # 0 for background, 1 for polyp\n",
    "        \n",
    "        # Apply transforms\n",
    "        # 'image' will be key for image, 'mask' for mask\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        \n",
    "        image_tensor = augmented['image']\n",
    "        mask_tensor = augmented['mask']\n",
    "        \n",
    "        # Add channel dimension to mask (H, W) -> (1, H, W)\n",
    "        # and convert from int64 to float for the loss function\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).float()\n",
    "        \n",
    "        return image_tensor, mask_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d58a0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 700 images, Val: 300 images\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATH COLLECTION AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple file existence check\n",
    "if not os.path.exists('train.txt') or not os.path.exists('val.txt'):\n",
    "    raise FileNotFoundError('train.txt or val.txt missing.')\n",
    "\n",
    "base_path = Path(config.DATASET_PATH)\n",
    "image_paths = sorted(list((base_path / config.IMAGE_DIR).glob('*.jpg')))\n",
    "mask_paths = sorted(list((base_path / config.MASK_DIR).glob('*.jpg')))\n",
    "\n",
    "if len(image_paths) != len(mask_paths):\n",
    "    raise ValueError(\"Mismatch between number of images and masks.\")\n",
    "\n",
    "# Read train/val lists\n",
    "with open('train.txt', 'r') as f:\n",
    "    train_stems = {line.strip() for line in f}\n",
    "with open('val.txt', 'r') as f:\n",
    "    val_stems = {line.strip() for line in f}\n",
    "\n",
    "# Split dataset according to txt files\n",
    "train_images = [p for p in image_paths if p.stem in train_stems]\n",
    "train_masks = [p for p in mask_paths if p.stem in train_stems]\n",
    "val_images = [p for p in image_paths if p.stem in val_stems]\n",
    "val_masks = [p for p in mask_paths if p.stem in val_stems]\n",
    "\n",
    "print(f\"Train: {len(train_images)} images, Val: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0814aa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Input shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL CALL\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_call():\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "    print(\"Model created successfully!\")\n",
    "    \n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "test_model_call()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccf1d45",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "979e6753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68aa02b",
   "metadata": {},
   "source": [
    "# U-Net model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channel=3, out_channel=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        for feature in features: \n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1]*2)\n",
    "\n",
    "        for features in reversed(features):\n",
    "            self.ups.append(nn.ConvTranspose2d(features*2, feature, kernel_size=2, stride=2))\n",
    "            self.ups.append(DoubleConv(feature*2, feature))\n",
    "\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "    \n",
    "     # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        # Decoder\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "            \n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = TF.resize(x, size=skip_connection.shape[2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "        \n",
    "        return self.final_conv(x)\n",
    "\n",
    "print(\"U-Net model defined (standard architecture)\")\n",
    "\n",
    "                 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208156d2",
   "metadata": {},
   "source": [
    "# Dataset class Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e88d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset with Albumentations augmentation\n",
    "    \n",
    "    EXPERIMENT: Training uses augmentation, validation doesn't\n",
    "    This tests if augmentation helps the model generalize better\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, target_size=256, use_augmentations=False):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        \n",
    "        if use_augmentations:\n",
    "            \n",
    "            \n",
    "            # TRAINING: WITH AUGMENTATION \n",
    "            \n",
    "            self.transform = A.Compose([\n",
    "                # Preprocessing (required)\n",
    "                A.Resize(height=target_size, width=target_size, \n",
    "                         interpolation=cv2.INTER_LINEAR),\n",
    "                \n",
    "                # GEOMETRIC AUGMENTATIONS\n",
    "                A.HorizontalFlip(p=0.5),        # Flip left-right\n",
    "                A.VerticalFlip(p=0.5),          # Flip top-bottom\n",
    "                A.Rotate(limit=30, interpolation=cv2.INTER_LINEAR, \n",
    "                         border_mode=cv2.BORDER_CONSTANT, p=0.5),  # Rotate\n",
    "                A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=0.3),  # Deform\n",
    "                A.GridDistortion(p=0.3),        # Distort grid\n",
    "                \n",
    "                # COLOR AUGMENTATIONS\n",
    "                A.ColorJitter(brightness=0.3, contrast=0.3, \n",
    "                              saturation=0.3, hue=0.1, p=0.5),  # Change colors\n",
    "                \n",
    "                # NOISE\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),  # Add noise\n",
    "                \n",
    "                # Convert to tensor\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "            print(\"Training augmentation enabled (8 techniques)\")\n",
    "            \n",
    "        else:\n",
    "            # VALIDATION: NO AUGMENTATION (BASELINE)\n",
    "           self.transform = A.Compose([\n",
    "                A.Resize(height=target_size, width=target_size, \n",
    "                         interpolation=cv2.INTER_LINEAR),\n",
    "                ToTensorV2()\n",
    "            ])\n",
    "        print(\"Validation: no augmentation\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = np.array(Image.open(self.image_paths[idx]).convert('RGB'))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert('L'))\n",
    "        \n",
    "        # Binarize mask (0 or 1)\n",
    "        mask = (mask > 128).astype(np.uint8)\n",
    "        \n",
    "        # Apply augmentation (same transforms to both image and mask!)\n",
    "        augmented = self.transform(image=image, mask=mask)\n",
    "        image_tensor = augmented['image']\n",
    "        mask_tensor = augmented['mask']\n",
    "        \n",
    "        # Add channel dimension to mask\n",
    "        mask_tensor = mask_tensor.unsqueeze(0).float()\n",
    "        \n",
    "        return image_tensor, mask_tensor\n",
    "\n",
    "print(\"Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728094d8",
   "metadata": {},
   "source": [
    "# Load dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e8adb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check files exist\n",
    "if not os.path.exists('train.txt') or not os.path.exists('val.txt'):\n",
    "    raise FileNotFoundError('train.txt or val.txt missing!')\n",
    "\n",
    "# Get all image and mask paths\n",
    "base_path = Path(config.DATASET_PATH)\n",
    "image_paths = sorted(list((base_path / config.IMAGE_DIR).glob('*.jpg')))\n",
    "mask_paths = sorted(list((base_path / config.MASK_DIR).glob('*.jpg')))\n",
    "\n",
    "if len(image_paths) != len(mask_paths):\n",
    "    raise ValueError(\"Mismatch between images and masks!\")\n",
    "\n",
    "print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# Read train/val split\n",
    "with open('train.txt', 'r') as f:\n",
    "    train_stems = {line.strip() for line in f}\n",
    "with open('val.txt', 'r') as f:\n",
    "    val_stems = {line.strip() for line in f}\n",
    "\n",
    "# Split dataset\n",
    "train_images = [p for p in image_paths if p.stem in train_stems]\n",
    "train_masks = [p for p in mask_paths if p.stem in train_stems]\n",
    "val_images = [p for p in image_paths if p.stem in val_stems]\n",
    "val_masks = [p for p in mask_paths if p.stem in val_stems]\n",
    "\n",
    "# Print split info\n",
    "print(f\"Training:   {len(train_images)} images ({len(train_images)/(len(train_images)+len(val_images))*100:.1f}%)\")\n",
    "print(f\"Validation: {len(val_images)} images ({len(val_images)/(len(train_images)+len(val_images))*100:.1f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f4f42",
   "metadata": {},
   "source": [
    "# Create dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6dcc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCreating datasets...\")\n",
    "\n",
    "# Training dataset WITH AUGMENTATION\n",
    "train_dataset = KvasirDataset(\n",
    "    train_images, \n",
    "    train_masks, \n",
    "    target_size=config.TARGET_SIZE,\n",
    "    use_augmentations=True  # ← THE KEY! This enables augmentation\n",
    ")\n",
    "\n",
    "# Validation dataset WITHOUT AUGMENTATION\n",
    "val_dataset = KvasirDataset(\n",
    "    val_images, \n",
    "    val_masks, \n",
    "    target_size=config.TARGET_SIZE,\n",
    "    use_augmentations=False  # ← No augmentation for validation\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154e399f",
   "metadata": {},
   "source": [
    "# Metrics and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9b1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    Dice Coefficient - measures overlap (0-1, higher is better)\n",
    "    Main metric for segmentation tasks\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "    \n",
    "    dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"\n",
    "    IoU (Intersection over Union) - alternative overlap metric (0-1, higher is better)\n",
    "    \"\"\"\n",
    "    pred = torch.sigmoid(pred)\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    intersection = (pred * target).sum(dim=(2, 3))\n",
    "    union = pred.sum(dim=(2, 3)) + target.sum(dim=(2, 3)) - intersection\n",
    "    \n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou.mean()\n",
    "\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Combined Dice + BCE Loss\n",
    "    Best for segmentation - handles class imbalance well\n",
    "    \"\"\"\n",
    "    def __init__(self, weight_dice=0.5, weight_bce=0.5):\n",
    "        super().__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_bce = weight_bce\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        \n",
    "        pred_sigmoid = torch.sigmoid(pred)\n",
    "        smooth = 1e-6\n",
    "        intersection = (pred_sigmoid * target).sum(dim=(2, 3))\n",
    "        union = pred_sigmoid.sum(dim=(2, 3)) + target.sum(dim=(2, 3))\n",
    "        dice_loss = 1 - (2.0 * intersection + smooth) / (union + smooth)\n",
    "        dice_loss = dice_loss.mean()\n",
    "        \n",
    "        return self.weight_bce * bce_loss + self.weight_dice * dice_loss\n",
    "\n",
    "print(\"Metrics and loss functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a126be",
   "metadata": {},
   "source": [
    "# Training and validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68006883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training')\n",
    "    for images, masks in pbar:\n",
    "        # Move data to GPU/CPU\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(images)  # Get predictions\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()  # Calculate gradients\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice_coefficient(outputs, masks).item()\n",
    "        total_iou += iou_score(outputs, masks).item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'dice': f'{dice_coefficient(outputs, masks).item():.4f}'\n",
    "        })\n",
    "    \n",
    "    # Return average metrics\n",
    "    return (total_loss / len(dataloader), \n",
    "            total_dice / len(dataloader), \n",
    "            total_iou / len(dataloader))\n",
    "\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    total_iou = 0\n",
    "    \n",
    "    with torch.no_grad():  # Don't calculate gradients (saves memory)\n",
    "        pbar = tqdm(dataloader, desc='Validation')\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_dice += dice_coefficient(outputs, masks).item()\n",
    "            total_iou += iou_score(outputs, masks).item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'dice': f'{dice_coefficient(outputs, masks).item():.4f}'\n",
    "            })\n",
    "    \n",
    "    return (total_loss / len(dataloader), \n",
    "            total_dice / len(dataloader), \n",
    "            total_iou / len(dataloader))\n",
    "\n",
    "print(\"Training and validation functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa13099",
   "metadata": {},
   "source": [
    "# MAIN TRAINING LOOP (WITH LR SCHEDULING + EARLY STOPPING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3dc8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize model\n",
    "model = UNet(\n",
    "    in_channels=config.IN_CHANNELS,\n",
    "    out_channels=config.OUT_CHANNELS,\n",
    "    features=config.FEATURES  # Standard U-Net architecture (not modified)\n",
    ").to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = DiceBCELoss(weight_dice=0.5, weight_bce=0.5)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
    "\n",
    "#  LEARNING RATE SCHEDULER (REQUIRED!)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max',  # Maximize Dice score\n",
    "    factor=config.SCHEDULER_FACTOR,  # Reduce by 90%\n",
    "    patience=config.SCHEDULER_PATIENCE,  # Wait 3 epochs\n",
    "    verbose=True  # Print when LR changes\n",
    ")\n",
    "\n",
    "# Training history\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_dice': [],\n",
    "    'train_iou': [],\n",
    "    'val_loss': [],\n",
    "    'val_dice': [],\n",
    "    'val_iou': [],\n",
    "    'lr': []\n",
    "}\n",
    "\n",
    "#  EARLY STOPPING (REQUIRED!)\n",
    "best_dice = 0\n",
    "patience_counter = 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"STARTING TRAINING WITH DATA AUGMENTATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Image size: {config.TARGET_SIZE}x{config.TARGET_SIZE}\")\n",
    "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
    "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
    "print(f\"Max epochs: {config.NUM_EPOCHS}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(config.NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Training phase\n",
    "    train_loss, train_dice, train_iou = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    \n",
    "    # Validation phase\n",
    "    val_loss, val_dice, val_iou = validate_epoch(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    #  UPDATE LEARNING RATE (REQUIRED!)\n",
    "    scheduler.step(val_dice)  # Reduce LR if Dice doesn't improve\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_dice'].append(train_dice)\n",
    "    history['train_iou'].append(train_iou)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_dice'].append(val_dice)\n",
    "    history['val_iou'].append(val_iou)\n",
    "    history['lr'].append(current_lr)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nTrain - Loss: {train_loss:.4f}, Dice: {train_dice:.4f}, IoU: {train_iou:.4f}\")\n",
    "    print(f\"Val   - Loss: {val_loss:.4f}, Dice: {val_dice:.4f}, IoU: {val_iou:.4f}\")\n",
    "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
    "    \n",
    "    # EARLY STOPPING LOGIC (REQUIRED!)\n",
    "    if val_dice > best_dice:\n",
    "        # Validation improved!\n",
    "        best_dice = val_dice\n",
    "        patience_counter = 0  # Reset counter\n",
    "        \n",
    "        # Save best model\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_dice': val_dice,\n",
    "            'val_iou': val_iou,\n",
    "        }, 'best_model_augmented.pth')\n",
    "        print(f\"✓ SAVED best model (Dice: {val_dice:.4f})\")\n",
    "    else:\n",
    "        # No improvement\n",
    "        patience_counter += 1\n",
    "        print(f\"  No improvement ({patience_counter}/{config.EARLY_STOPPING_PATIENCE})\")\n",
    "    \n",
    "    # CHECK EARLY STOPPING (REQUIRED!)\n",
    "    if patience_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "        print(f\"\\n⚠ EARLY STOPPING triggered after {epoch+1} epochs\")\n",
    "        print(f\"  No improvement for {config.EARLY_STOPPING_PATIENCE} epochs\")\n",
    "        break\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"TRAINING COMPLETE! Best Dice: {best_dice:.4f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecbdee",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a32880c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Loss plot\n",
    "axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')\n",
    "axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='s')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training and Validation Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Dice plot\n",
    "axes[0, 1].plot(history['train_dice'], label='Train Dice', marker='o')\n",
    "axes[0, 1].plot(history['val_dice'], label='Val Dice', marker='s')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Dice Score')\n",
    "axes[0, 1].set_title('Dice Score (Main Metric)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# IoU plot\n",
    "axes[1, 0].plot(history['train_iou'], label='Train IoU', marker='o')\n",
    "axes[1, 0].plot(history['val_iou'], label='Val IoU', marker='s')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('IoU Score')\n",
    "axes[1, 0].set_title('IoU Score')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Learning rate plot\n",
    "axes[1, 1].plot(history['lr'], marker='o', color='red')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Learning Rate')\n",
    "axes[1, 1].set_title('Learning Rate Schedule')\n",
    "axes[1, 1].set_yscale('log')  # Log scale to see reductions clearly\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results_augmented.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Plots saved as 'training_results_augmented.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac24c123",
   "metadata": {},
   "source": [
    "# Save results (aner ikke om dette trengs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cec34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history to CSV\n",
    "history_df = pd.DataFrame(history)\n",
    "history_df.to_csv('training_history_augmented.csv', index=False)\n",
    "print(\"History saved as 'training_history_augmented.csv'\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ALL RESULTS SAVED!\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  1. best_model_augmented.pth          - Best model checkpoint\")\n",
    "print(\"  2. training_history_augmented.csv    - All metrics per epoch\")\n",
    "print(\"  3. training_results_augmented.png    - Training curves\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nTASK 2 COMPLETE!\")\n",
    "print(\"Your experiment: Training WITH augmentation (8 techniques)\")\n",
    "print(\"Focus of report: How augmentation improves generalization\")\n",
    "print(\"=\" * 70)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

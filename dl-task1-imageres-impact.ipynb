{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5aac88b",
   "metadata": {},
   "source": [
    "Task 1: Impact of image resolution on the final outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879df65",
   "metadata": {},
   "source": [
    "Task 1: Impact of Image Resolution on U-Net Segmentation Performance\n",
    "Dataset: Kvasir-SEG (Polyp Segmentation)\n",
    "Workflow: Original Image -> Scale to [512, 256, 128, 64] -> Rescale to 256x256 -> U-Net\n",
    "\n",
    "This notebook investigates how different input resolutions affect segmentation quality.\n",
    "\n",
    "(Accuracy, IOU, F1, DICE, MCC, precision, sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========== IGNORE =========================================================\n",
    "# shuffle should be true for train, false for validation\n",
    "# use semi-transparent masks when printing images with mask overlay\n",
    "# visual representations of predictions alongside metrics (numbers). not separate\n",
    "# paper should include tables, images, curves\n",
    "# preprocess mask? totensor will give values between 0 and 1. Not 0, 1. (bad)\n",
    "# create model in separate .py, reuse in other tasks\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f288b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "from model import UNet\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 1337\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "605518d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "train_file = 'train.txt'\n",
    "val_file = 'val.txt'\n",
    "\n",
    "class Config:\n",
    "    # Dataset paths\n",
    "    DATASET_PATH = \"data\\kvasir-seg\"\n",
    "    IMAGE_DIR = \"images\"\n",
    "    MASK_DIR = \"masks\"\n",
    "    \n",
    "    # Experiment parameters\n",
    "    RESOLUTIONS = [512, 256, 128, 64]\n",
    "    TARGET_SIZE = 256\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "    \n",
    "    # Model parameters\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 1\n",
    "    FEATURES = [64, 128, 256, 512]\n",
    "    \n",
    "    RESULTS_DIR = \"results_task1\"\n",
    "    \n",
    "config = Config()\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True) # is this needed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611e09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, resolution, target_size=256):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.resolution = resolution\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        \n",
    "        # Scale to test resolution (information loss)\n",
    "        image = TF.resize(image, (self.resolution, self.resolution), \n",
    "                         interpolation=Image.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.resolution, self.resolution), \n",
    "                        interpolation=Image.NEAREST)\n",
    "        \n",
    "        # Scale back to target size\n",
    "        image = TF.resize(image, (self.target_size, self.target_size), \n",
    "                         interpolation=Image.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.target_size, self.target_size), \n",
    "                        interpolation=Image.NEAREST)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "999a23a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 700 images, Val: 300 images\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATH COLLECTION AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Simple file existence check\n",
    "if not os.path.exists('train.txt') or not os.path.exists('val.txt'):\n",
    "    raise FileNotFoundError('train.txt or val.txt missing.')\n",
    "\n",
    "base_path = Path(config.DATASET_PATH)\n",
    "image_paths = sorted(list((base_path / config.IMAGE_DIR).glob('*.jpg')))\n",
    "mask_paths = sorted(list((base_path / config.MASK_DIR).glob('*.jpg')))\n",
    "\n",
    "if len(image_paths) != len(mask_paths):\n",
    "    raise ValueError(\"Mismatch between number of images and masks.\")\n",
    "\n",
    "# Read train/val lists\n",
    "with open('train.txt', 'r') as f:\n",
    "    train_stems = {line.strip() for line in f}\n",
    "with open('val.txt', 'r') as f:\n",
    "    val_stems = {line.strip() for line in f}\n",
    "\n",
    "# Split dataset according to txt files\n",
    "train_images = [p for p in image_paths if p.stem in train_stems]\n",
    "train_masks = [p for p in mask_paths if p.stem in train_stems]\n",
    "val_images = [p for p in image_paths if p.stem in val_stems]\n",
    "val_masks = [p for p in mask_paths if p.stem in val_stems]\n",
    "\n",
    "print(f\"Train: {len(train_images)} images, Val: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4d3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created successfully!\n",
      "Input shape: torch.Size([1, 3, 256, 256])\n",
      "Output shape: torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# MODEL CALL\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_call():\n",
    "    model = UNet(in_channels=3, out_channels=1).to(device)\n",
    "    print(\"Model created successfully!\")\n",
    "    \n",
    "    # Create a dummy input tensor\n",
    "    dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "    print(f\"Input shape: {dummy_input.shape}\")\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        output = model(dummy_input)\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "test_model_call()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

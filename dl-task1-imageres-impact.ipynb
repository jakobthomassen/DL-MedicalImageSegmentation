{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5aac88b",
   "metadata": {},
   "source": [
    "Task 1: Impact of image resolution on the final outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2879df65",
   "metadata": {},
   "source": [
    "Task 1: Impact of Image Resolution on U-Net Segmentation Performance\n",
    "Dataset: Kvasir-SEG (Polyp Segmentation)\n",
    "Workflow: Original Image -> Scale to [512, 256, 128, 64] -> Rescale to 256x256 -> U-Net\n",
    "\n",
    "This notebook investigates how different input resolutions affect segmentation quality.\n",
    "\n",
    "(Accuracy, IOU, F1, DICE, MCC, precision, sensitivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67e2376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd \n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "\n",
    "# Set random seeds\n",
    "SEED = 55\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605518d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "class Config:\n",
    "    # Dataset paths (update these)\n",
    "    DATASET_PATH = \"data\\kvasir-seg\"\n",
    "    IMAGE_DIR = \"images\"\n",
    "    MASK_DIR = \"masks\"\n",
    "    \n",
    "    # Experiment parameters\n",
    "    RESOLUTIONS = [512, 256, 128, 64]\n",
    "    TARGET_SIZE = 256\n",
    "    \n",
    "    # Training parameters\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 25\n",
    "    LEARNING_RATE = 1e-4\n",
    "    TRAIN_SPLIT = 0.7\n",
    "    \n",
    "    # Model parameters\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 1\n",
    "    FEATURES = [64, 128, 256, 512]\n",
    "    \n",
    "    RESULTS_DIR = \"results_task1\"\n",
    "    \n",
    "config = Config()\n",
    "os.makedirs(config.RESULTS_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611e09ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATASET CLASS\n",
    "# ============================================================================\n",
    "\n",
    "class KvasirDataset(Dataset):\n",
    "    \"\"\"Kvasir-SEG Dataset with resolution scaling\"\"\"\n",
    "    \n",
    "    def __init__(self, image_paths, mask_paths, resolution, target_size=256):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.resolution = resolution\n",
    "        self.target_size = target_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
    "        mask = Image.open(self.mask_paths[idx]).convert('L')\n",
    "        \n",
    "        # Scale to test resolution (information loss)\n",
    "        image = TF.resize(image, (self.resolution, self.resolution), \n",
    "                         interpolation=Image.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.resolution, self.resolution), \n",
    "                        interpolation=Image.NEAREST)\n",
    "        \n",
    "        # Scale back to target size\n",
    "        image = TF.resize(image, (self.target_size, self.target_size), \n",
    "                         interpolation=Image.BILINEAR)\n",
    "        mask = TF.resize(mask, (self.target_size, self.target_size), \n",
    "                        interpolation=Image.NEAREST)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        image = TF.to_tensor(image)\n",
    "        mask = TF.to_tensor(mask)\n",
    "        mask = (mask > 0.5).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b1d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Info ---\n",
      "Total samples found: 1000\n",
      "Train samples: 699\n",
      "Test samples: 301\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PATH COLLECTION AND VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    # Use glob for real path collection\n",
    "    base_path = Path(config.DATASET_PATH)\n",
    "    image_paths = sorted(list((base_path / config.IMAGE_DIR).glob('*.jpg')))\n",
    "    mask_paths = sorted(list((base_path / config.MASK_DIR).glob('*.jpg')))\n",
    "\n",
    "    if not image_paths or not mask_paths:\n",
    "        raise FileNotFoundError(f\"Could not find images or masks in {config.DATASET_PATH}. Simulating.\")\n",
    "\n",
    "except (FileNotFoundError, NotADirectoryError, OSError):\n",
    "    # Fallback to simulation if the local path structure is not present\n",
    "    num_samples = 100\n",
    "    image_paths = [Path(f\"/simulated/path/images/{i:03d}.jpg\") for i in range(num_samples)]\n",
    "    mask_paths = [Path(f\"/simulated/path/masks/{i:03d}.jpg\") for i in range(num_samples)]\n",
    "\n",
    "# Split the data\n",
    "if len(image_paths) != len(mask_paths):\n",
    "    raise ValueError(\"Error: Number of images and masks do not match.\")\n",
    "\n",
    "train_images, test_images, train_masks, test_masks = train_test_split(\n",
    "    image_paths, \n",
    "    mask_paths, \n",
    "    test_size=1 - config.TRAIN_SPLIT, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# --- SIMPLE VALIDATION ---\n",
    "print(\"\\n--- Dataset Info ---\")\n",
    "print(f\"Total samples found: {len(image_paths)}\")\n",
    "print(f\"Train samples: {len(train_images)}\")\n",
    "print(f\"Test samples: {len(test_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a4d3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# U-NET ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Conv -> BN -> ReLU -> Conv -> BN -> ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"U-Net for Medical Image Segmentation\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=1, features=[64, 128, 256, 512]):\n",
    "        super(UNet, self).__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Encoder\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "        \n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        \n",
    "        # Decoder\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "        \n",
    "        # Final output\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        \n",
    "        # Encoder\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "        \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "        \n",
    "        # Decoder\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip = skip_connections[idx // 2]\n",
    "            \n",
    "            # Handle size mismatch\n",
    "            if x.shape != skip.shape:\n",
    "                x = TF.resize(x, size=skip.shape[2:])\n",
    "            \n",
    "            concat_skip = torch.cat((skip, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "        \n",
    "        return torch.sigmoid(self.final_conv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dcde08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# METRICS\n",
    "# ============================================================================\n",
    "\n",
    "def dice_coefficient(pred, target, smooth=1e-6):\n",
    "    \"\"\"Dice Coefficient (F1 Score for segmentation)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    return (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"Intersection over Union (Jaccard Index)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection + smooth) / (union + smooth)\n",
    "\n",
    "def pixel_accuracy(pred, target):\n",
    "    \"\"\"Pixel-wise Accuracy\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    correct = (pred == target).sum()\n",
    "    return correct.float() / target.numel()\n",
    "\n",
    "def precision_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"Precision: TP / (TP + FP)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    tp = (pred * target).sum()\n",
    "    return (tp + smooth) / (pred.sum() + smooth)\n",
    "\n",
    "def recall_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"Recall (Sensitivity): TP / (TP + FN)\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    tp = (pred * target).sum()\n",
    "    return (tp + smooth) / (target.sum() + smooth)\n",
    "\n",
    "def f1_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"F1 Score: 2 * (Precision * Recall) / (Precision + Recall)\"\"\"\n",
    "    prec = precision_score(pred, target, smooth)\n",
    "    rec = recall_score(pred, target, smooth)\n",
    "    return 2 * (prec * rec) / (prec + rec + smooth)\n",
    "\n",
    "def mcc_score(pred, target, smooth=1e-6):\n",
    "    \"\"\"Matthews Correlation Coefficient\"\"\"\n",
    "    pred = pred.view(-1)\n",
    "    target = target.view(-1)\n",
    "    \n",
    "    tp = (pred * target).sum()\n",
    "    tn = ((1 - pred) * (1 - target)).sum()\n",
    "    fp = (pred * (1 - target)).sum()\n",
    "    fn = ((1 - pred) * target).sum()\n",
    "    \n",
    "    numerator = tp * tn - fp * fn\n",
    "    denominator = torch.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "    \n",
    "    return numerator / (denominator + smooth)\n",
    "\n",
    "def compute_all_metrics(pred, target, threshold=0.5):\n",
    "    \"\"\"Compute all metrics\"\"\"\n",
    "    pred_binary = (pred > threshold).float()\n",
    "    \n",
    "    metrics = {\n",
    "        'dice': dice_coefficient(pred_binary, target).item(),\n",
    "        'iou': iou_score(pred_binary, target).item(),\n",
    "        'f1': f1_score(pred_binary, target).item(),\n",
    "        'accuracy': pixel_accuracy(pred_binary, target).item(),\n",
    "        'precision': precision_score(pred_binary, target).item(),\n",
    "        'recall': recall_score(pred_binary, target).item(),\n",
    "        'mcc': mcc_score(pred_binary, target).item()\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47842a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING AND EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "class DiceBCELoss(nn.Module):\n",
    "    \"\"\"Combined Dice Loss and BCE Loss\"\"\"\n",
    "    def __init__(self, weight_dice=0.5, weight_bce=0.5):\n",
    "        super(DiceBCELoss, self).__init__()\n",
    "        self.weight_dice = weight_dice\n",
    "        self.weight_bce = weight_bce\n",
    "        self.bce = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, pred, target):\n",
    "        # Dice Loss\n",
    "        dice = dice_coefficient(pred, target)\n",
    "        dice_loss = 1 - dice\n",
    "        \n",
    "        # BCE Loss\n",
    "        bce_loss = self.bce(pred, target)\n",
    "        \n",
    "        return self.weight_dice * dice_loss + self.weight_bce * bce_loss\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_dice = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc='Training')\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        with torch.no_grad():\n",
    "            dice = dice_coefficient(outputs > 0.5, masks)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_dice += dice.item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': loss.item(), 'dice': dice.item()})\n",
    "    \n",
    "    return total_loss / len(loader), total_dice / len(loader)\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    \"\"\"Validate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_metrics = {\n",
    "        'dice': [], 'iou': [], 'f1': [], 'accuracy': [],\n",
    "        'precision': [], 'recall': [], 'mcc': []\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(loader, desc='Validation')\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Compute metrics for each sample\n",
    "            for i in range(outputs.size(0)):\n",
    "                metrics = compute_all_metrics(outputs[i], masks[i])\n",
    "                for key, value in metrics.items():\n",
    "                    all_metrics[key].append(value)\n",
    "    \n",
    "    # Average metrics\n",
    "    avg_metrics = {key: np.mean(values) for key, values in all_metrics.items()}\n",
    "    avg_metrics['loss'] = total_loss / len(loader)\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs, device):\n",
    "    \"\"\"Complete training loop\"\"\"\n",
    "    criterion = DiceBCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='max', patience=5, factor=0.5\n",
    "    )\n",
    "    \n",
    "    history = {\n",
    "        'train_loss': [], 'train_dice': [],\n",
    "        'val_loss': [], 'val_dice': [], 'val_iou': [],\n",
    "        'val_f1': [], 'val_accuracy': [], 'val_precision': [],\n",
    "        'val_recall': [], 'val_mcc': []\n",
    "    }\n",
    "    \n",
    "    best_dice = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_dice = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_dice'].append(train_dice)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_dice'].append(val_metrics['dice'])\n",
    "        history['val_iou'].append(val_metrics['iou'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "        history['val_precision'].append(val_metrics['precision'])\n",
    "        history['val_recall'].append(val_metrics['recall'])\n",
    "        history['val_mcc'].append(val_metrics['mcc'])\n",
    "        \n",
    "        # Scheduler step\n",
    "        scheduler.step(val_metrics['dice'])\n",
    "        \n",
    "        # Print metrics\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Dice: {train_dice:.4f}')\n",
    "        print(f'Val Loss: {val_metrics[\"loss\"]:.4f}')\n",
    "        print(f'Val Dice: {val_metrics[\"dice\"]:.4f}, IoU: {val_metrics[\"iou\"]:.4f}')\n",
    "        print(f'Val F1: {val_metrics[\"f1\"]:.4f}, Acc: {val_metrics[\"accuracy\"]:.4f}')\n",
    "        print(f'Val Precision: {val_metrics[\"precision\"]:.4f}, Recall: {val_metrics[\"recall\"]:.4f}')\n",
    "        print(f'Val MCC: {val_metrics[\"mcc\"]:.4f}')\n",
    "        \n",
    "        # Save best model\n",
    "        if val_metrics['dice'] > best_dice:\n",
    "            best_dice = val_metrics['dice']\n",
    "            print(f'New best Dice: {best_dice:.4f}')\n",
    "    \n",
    "    return history, best_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ab63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1000 images and 1000 masks\n",
      "Training samples: 699\n",
      "Validation samples: 301\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DATA LOADING\n",
    "# ============================================================================\n",
    "def load_dataset_paths(base_path, image_dir, mask_dir):\n",
    "    \"\"\"Load image and mask file paths\"\"\"\n",
    "    image_path = Path(base_path) / image_dir\n",
    "    mask_path = Path(base_path) / mask_dir\n",
    "    \n",
    "    image_files = sorted(list(image_path.glob('*.jpg')) + list(image_path.glob('*.png')))\n",
    "    mask_files = sorted(list(mask_path.glob('*.jpg')) + list(mask_path.glob('*.png')))\n",
    "    \n",
    "    print(f\"Found {len(image_files)} images and {len(mask_files)} masks\")\n",
    "    \n",
    "    return image_files, mask_files\n",
    "\n",
    "# Load dataset\n",
    "image_paths, mask_paths = load_dataset_paths(\n",
    "    config.DATASET_PATH, config.IMAGE_DIR, config.MASK_DIR\n",
    ")\n",
    "\n",
    "# Train/val split (70/30)\n",
    "train_images, val_images, train_masks, val_masks = train_test_split(\n",
    "    image_paths, mask_paths, test_size=(1 - config.TRAIN_SPLIT), random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_images)}\")\n",
    "print(f\"Validation samples: {len(val_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53edb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Testing Resolution: 512x512\n",
      "============================================================\n",
      "Model parameters: 31,037,633\n",
      "\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 2/88 [00:26<18:55, 13.20s/it, loss=0.769, dice=0.206]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m history, best_dice = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[32m     53\u001b[39m results[resolution] = {\n\u001b[32m     54\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m'\u001b[39m: history,\n\u001b[32m     55\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbest_dice\u001b[39m\u001b[33m'\u001b[39m: best_dice,\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     }\n\u001b[32m     65\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, num_epochs, device)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m train_loss, train_dice = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m    111\u001b[39m val_metrics = validate(model, val_loader, criterion, device)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Backward\u001b[39;00m\n\u001b[32m     39\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m optimizer.step()\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pizza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pizza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Pizza\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXPERIMENT: TEST DIFFERENT RESOLUTIONS\n",
    "# ============================================================================\n",
    "\n",
    "results = {}\n",
    "\n",
    "for resolution in config.RESOLUTIONS:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing Resolution: {resolution}x{resolution}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = KvasirDataset(\n",
    "        train_images, train_masks, \n",
    "        resolution=resolution, \n",
    "        target_size=config.TARGET_SIZE\n",
    "    )\n",
    "    val_dataset = KvasirDataset(\n",
    "        val_images, val_masks, \n",
    "        resolution=resolution, \n",
    "        target_size=config.TARGET_SIZE\n",
    "    )\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=0\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config.BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = UNet(\n",
    "        in_channels=config.IN_CHANNELS,\n",
    "        out_channels=config.OUT_CHANNELS,\n",
    "        features=config.FEATURES\n",
    "    ).to(device)\n",
    "    \n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    \n",
    "    # Train\n",
    "    history, best_dice = train_model(\n",
    "        model, train_loader, val_loader, config.NUM_EPOCHS, device\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results[resolution] = {\n",
    "        'history': history,\n",
    "        'best_dice': best_dice,\n",
    "        'final_metrics': {\n",
    "            'dice': history['val_dice'][-1],\n",
    "            'iou': history['val_iou'][-1],\n",
    "            'f1': history['val_f1'][-1],\n",
    "            'accuracy': history['val_accuracy'][-1],\n",
    "            'precision': history['val_precision'][-1],\n",
    "            'recall': history['val_recall'][-1],\n",
    "            'mcc': history['val_mcc'][-1]\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \n",
    "               f\"{config.RESULTS_DIR}/model_res{resolution}.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION AND ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "# Plot training curves for all resolutions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for resolution in config.RESOLUTIONS:\n",
    "    history = results[resolution]['history']\n",
    "    \n",
    "    # Loss\n",
    "    axes[0, 0].plot(history['train_loss'], label=f'{resolution}x{resolution}')\n",
    "    axes[0, 1].plot(history['val_loss'], label=f'{resolution}x{resolution}')\n",
    "    \n",
    "    # Dice\n",
    "    axes[1, 0].plot(history['train_dice'], label=f'{resolution}x{resolution}')\n",
    "    axes[1, 1].plot(history['val_dice'], label=f'{resolution}x{resolution}')\n",
    "\n",
    "axes[0, 0].set_title('Training Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "axes[0, 1].set_title('Validation Loss')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "axes[1, 0].set_title('Training Dice Score')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Dice')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "axes[1, 1].set_title('Validation Dice Score')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Dice')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.RESULTS_DIR}/training_curves.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Compare all metrics across resolutions\n",
    "metrics_df = pd.DataFrame([\n",
    "    {\n",
    "        'Resolution': res,\n",
    "        'Dice': results[res]['final_metrics']['dice'],\n",
    "        'IoU': results[res]['final_metrics']['iou'],\n",
    "        'F1': results[res]['final_metrics']['f1'],\n",
    "        'Accuracy': results[res]['final_metrics']['accuracy'],\n",
    "        'Precision': results[res]['final_metrics']['precision'],\n",
    "        'Recall': results[res]['final_metrics']['recall'],\n",
    "        'MCC': results[res]['final_metrics']['mcc']\n",
    "    }\n",
    "    for res in config.RESOLUTIONS\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS: Impact of Resolution on Segmentation\")\n",
    "print(\"=\"*60)\n",
    "print(metrics_df.to_string(index=False))\n",
    "\n",
    "# Save results\n",
    "metrics_df.to_csv(f\"{config.RESULTS_DIR}/metrics_comparison.csv\", index=False)\n",
    "\n",
    "# Heatmap of metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    metrics_df.set_index('Resolution').T, \n",
    "    annot=True, fmt='.4f', cmap='RdYlGn', \n",
    "    vmin=0, vmax=1, ax=ax, cbar_kws={'label': 'Score'}\n",
    ")\n",
    "ax.set_title('Segmentation Metrics Across Different Resolutions', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Resolution', fontsize=12)\n",
    "ax.set_ylabel('Metric', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.RESULTS_DIR}/metrics_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Bar plots for each metric\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "metric_names = ['Dice', 'IoU', 'F1', 'Accuracy', 'Precision', 'Recall', 'MCC']\n",
    "\n",
    "for idx, metric in enumerate(metric_names):\n",
    "    axes[idx].bar(\n",
    "        metrics_df['Resolution'].astype(str), \n",
    "        metrics_df[metric],\n",
    "        color=sns.color_palette('viridis', len(config.RESOLUTIONS))\n",
    "    )\n",
    "    axes[idx].set_title(metric, fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Resolution')\n",
    "    axes[idx].set_ylabel('Score')\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(metrics_df[metric]):\n",
    "        axes[idx].text(i, v + 0.02, f'{v:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "# Hide extra subplot\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Comparison of Segmentation Metrics Across Resolutions', \n",
    "             fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{config.RESULTS_DIR}/metrics_barplots.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e07e0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ============================================================================\n",
    "# QUALITATIVE ANALYSIS: Visualize Predictions\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_predictions(model, dataset, device, num_samples=5):\n",
    "    \"\"\"Visualize model predictions\"\"\"\n",
    "    model.eval()\n",
    "    indices = np.random.choice(len(dataset), num_samples, replace=False)\n",
    "    \n",
    "    fig, axes = plt.subplots(num_samples, 4, figsize=(16, 4*num_samples))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(indices):\n",
    "            image, mask = dataset[idx]\n",
    "            image_input = image.unsqueeze(0).to(device)\n",
    "            \n",
    "            pred = model(image_input)\n",
    "            pred = pred.squeeze().cpu().numpy()\n",
    "            pred_binary = (pred > 0.5).astype(np.float32)\n",
    "            \n",
    "            # Display\n",
    "            image_np = image.permute(1, 2, 0).numpy()\n",
    "            mask_np = mask.squeeze().numpy()\n",
    "            \n",
    "            axes[i, 0].imshow(image_np)\n",
    "            axes[i, 0].set_title('Input Image')\n",
    "            axes[i, 0].axis('off')\n",
    "            \n",
    "            axes[i, 1].imshow(mask_np, cmap='gray')\n",
    "            axes[i, 1].set_title('Ground Truth')\n",
    "            axes[i, 1].axis('off')\n",
    "            \n",
    "            axes[i, 2].imshow(pred, cmap='gray')\n",
    "            axes[i, 2].set_title('Prediction (Probability)')\n",
    "            axes[i, 2].axis('off')\n",
    "            \n",
    "            axes[i, 3].imshow(pred_binary, cmap='gray')\n",
    "            axes[i, 3].set_title('Prediction (Binary)')\n",
    "            axes[i, 3].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Visualize for each resolution\n",
    "for resolution in config.RESOLUTIONS:\n",
    "    print(f\"\\nGenerating visualizations for {resolution}x{resolution}...\")\n",
    "    \n",
    "    # Load model\n",
    "    model = UNet(\n",
    "        in_channels=config.IN_CHANNELS,\n",
    "        out_channels=config.OUT_CHANNELS,\n",
    "        features=config.FEATURES\n",
    "    ).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"{config.RESULTS_DIR}/model_res{resolution}.pth\", \n",
    "                   map_location=device)\n",
    "    )\n",
    "    \n",
    "    # Create dataset\n",
    "    val_dataset = KvasirDataset(\n",
    "        val_images, val_masks, \n",
    "        resolution=resolution, \n",
    "        target_size=config.TARGET_SIZE\n",
    "    )\n",
    "    \n",
    "    # Visualize\n",
    "    fig = visualize_predictions(model, val_dataset, device, num_samples=5)\n",
    "    fig.suptitle(f'Predictions at {resolution}x{resolution} Resolution', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.savefig(f\"{config.RESULTS_DIR}/predictions_res{resolution}.png\", \n",
    "                dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TASK 1 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"All results saved to: {config.RESULTS_DIR}/\")\n",
    "print(\"\\nKey Findings:\")\n",
    "print(f\"- Best Resolution: {metrics_df.loc[metrics_df['Dice'].idxmax(), 'Resolution']}\")\n",
    "print(f\"- Best Dice Score: {metrics_df['Dice'].max():.4f}\")\n",
    "print(f\"- Worst Resolution: {metrics_df.loc[metrics_df['Dice'].idxmin(), 'Resolution']}\")\n",
    "print(f\"- Worst Dice Score: {metrics_df['Dice'].min():.4f}\")\n",
    "print(f\"\\nDice Score Range: {metrics_df['Dice'].min():.4f} - {metrics_df['Dice'].max():.4f}\")\n",
    "print(f\"Performance Drop: {(metrics_df['Dice'].max() - metrics_df['Dice'].min()):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
